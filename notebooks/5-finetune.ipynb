{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Billy\\miniforge3\\envs\\homedepot\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import datasets, evaluation, InputExample, losses, models, SentenceTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Datasets from Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Product Data\n",
    "\n",
    "df_test = pd.read_csv('raw_data/test.csv', encoding = 'ISO-8859-1')\n",
    "df_solution = pd.read_csv('raw_data\\solution.csv', encoding = 'ISO-8859-1')\n",
    "df = df_test.merge(df_solution, how = 'left', on = 'id')\n",
    "\n",
    "df_desc = pd.read_csv('raw_data/product_descriptions.csv', encoding = 'ISO-8859-1')\n",
    "df_attr = pd.read_csv('raw_data/attributes.csv', encoding = 'ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Products Dataset creation\n",
    "df_prods = df[['product_uid', 'product_title']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Merge in descriptions\n",
    "df_prods = df_prods.merge(df_desc, how = 'left', on = 'product_uid')\n",
    "\n",
    "# Collect attributes\n",
    "attr_ids = []\n",
    "attr_dicts = []\n",
    "for id in df_attr['product_uid'].unique():\n",
    "    attr_ids.append(id)\n",
    "    attr_tmp = df_attr[df_attr['product_uid']==id]\n",
    "    if len(attr_tmp)>0:\n",
    "        attrs = {}\n",
    "        for index, row in attr_tmp.iterrows():\n",
    "            attrs[row['name']] = row['value']\n",
    "        attr_dicts.append(attrs)\n",
    "    else:\n",
    "        attr_dicts.append(None)\n",
    "\n",
    "df_attributes = pd.DataFrame({'product_uid': attr_ids,\n",
    "                              'product_attributes': attr_dicts})     \n",
    "\n",
    "# Merge in attributes\n",
    "df_prods = df_prods.merge(df_attributes, how = 'left', on = 'product_uid')\n",
    "\n",
    "df_prods.to_csv('processed_data/test_data/df_test_prods.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_dict(string):\n",
    "    try:\n",
    "        return ast.literal_eval(string)\n",
    "    except (SyntaxError, ValueError):\n",
    "        return None  # Handle cases where the string is not a valid dictionary representation\n",
    "\n",
    "def dict_to_string(my_dict):\n",
    "    result_str = \"\"\n",
    "    for key, value in my_dict.items():\n",
    "        result_str += str(key) + ' ' + str(value) + ' '\n",
    "    return result_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prods = pd.read_csv('processed_data/test_data/df_test_prods.csv')\n",
    "\n",
    "df_prods['product_attributes'] = df_prods['product_attributes'].apply(convert_to_dict)\n",
    "df_prods['product_attributes_string'] = [dict_to_string(x) if x is not None else x for x in df_prods['product_attributes']]\n",
    "df_prods['product_text_string'] = df_prods['product_title'].fillna('') + ' ' + df_prods['product_description'].fillna('') + ' ' + df_prods['product_attributes_string'].fillna('') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_id(text):\n",
    "    hash_object = hashlib.sha256(text.encode('utf-8'))  # SHA-256 example\n",
    "    hex_digest = hash_object.hexdigest()\n",
    "    return hex_digest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>search_term</th>\n",
       "      <th>query_id</th>\n",
       "      <th>has_relevant_results</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90 degree bracket</td>\n",
       "      <td>d9a0dddfafa498b8042cca1c26f061679dcb3a23840079...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>metal l brackets</td>\n",
       "      <td>84c87102bd2789d4d946b3924f319f466923a69c3cc373...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>simpson sku able</td>\n",
       "      <td>f15e1eda53817ef17b26ef45977492190f604db7a12ce4...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>simpson strong  ties</td>\n",
       "      <td>a74edca209bbc084e7d72b351203715898da0e5561f11b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>simpson strong tie hcc668</td>\n",
       "      <td>2107aa7f3726c800f22cf7375517e6a02ed82f772fcb81...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 search_term  \\\n",
       "0          90 degree bracket   \n",
       "1           metal l brackets   \n",
       "2           simpson sku able   \n",
       "3       simpson strong  ties   \n",
       "4  simpson strong tie hcc668   \n",
       "\n",
       "                                            query_id  has_relevant_results  \n",
       "0  d9a0dddfafa498b8042cca1c26f061679dcb3a23840079...                     1  \n",
       "1  84c87102bd2789d4d946b3924f319f466923a69c3cc373...                     1  \n",
       "2  f15e1eda53817ef17b26ef45977492190f604db7a12ce4...                     1  \n",
       "3  a74edca209bbc084e7d72b351203715898da0e5561f11b...                     1  \n",
       "4  2107aa7f3726c800f22cf7375517e6a02ed82f772fcb81...                     1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Query Data\n",
    "df_queries = pd.DataFrame(df['search_term'].unique()).rename(columns={0: 'search_term'})\n",
    "df_queries['query_id'] = [generate_id(x) for x in df_queries['search_term']]\n",
    "df_queries['has_relevant_results'] = df_queries['search_term'].isin(df[df['relevance']>0]['search_term']).astype(int)\n",
    "\n",
    "df_queries.to_csv('processed_data/test_data/df_test_queries.csv', index=False)\n",
    "\n",
    "df_queries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>product_uid</th>\n",
       "      <th>relevance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d9a0dddfafa498b8042cca1c26f061679dcb3a23840079...</td>\n",
       "      <td>100001</td>\n",
       "      <td>-1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>84c87102bd2789d4d946b3924f319f466923a69c3cc373...</td>\n",
       "      <td>100001</td>\n",
       "      <td>2.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f15e1eda53817ef17b26ef45977492190f604db7a12ce4...</td>\n",
       "      <td>100001</td>\n",
       "      <td>2.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a74edca209bbc084e7d72b351203715898da0e5561f11b...</td>\n",
       "      <td>100001</td>\n",
       "      <td>2.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2107aa7f3726c800f22cf7375517e6a02ed82f772fcb81...</td>\n",
       "      <td>100001</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            query_id  product_uid  relevance\n",
       "0  d9a0dddfafa498b8042cca1c26f061679dcb3a23840079...       100001      -1.00\n",
       "1  84c87102bd2789d4d946b3924f319f466923a69c3cc373...       100001       2.33\n",
       "2  f15e1eda53817ef17b26ef45977492190f604db7a12ce4...       100001       2.33\n",
       "3  a74edca209bbc084e7d72b351203715898da0e5561f11b...       100001       2.67\n",
       "4  2107aa7f3726c800f22cf7375517e6a02ed82f772fcb81...       100001       2.00"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Relevance\n",
    "\n",
    "df_relevance = df.merge(df_queries, how = 'left', on = 'search_term')\n",
    "df_relevance = df_relevance[['query_id', 'product_uid', 'relevance']]\n",
    "df_relevance.to_csv('processed_data/test_data/df_test_relevance.csv', index=False)\n",
    "df_relevance.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any query product pairs that are in our previously embedded set\n",
    "df_train_relevance = pd.read_csv('processed_data/df_relevance.csv')\n",
    "\n",
    "identified_duplicates = pd.merge(df_relevance, df_train_relevance[['query_id', 'product_uid']], \n",
    "                                  on=['query_id', 'product_uid'], how='inner')\n",
    "\n",
    "df_relevance = df_relevance[~df_relevance.index.isin(identified_duplicates.index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'Represent this sentence for searching relevant passages: '\n",
    "\n",
    "positive_df = df_relevance[df_relevance['relevance']==3].copy()\n",
    "positive_df = positive_df.merge(df_prods[['product_uid', 'product_text_string']], how = 'left', on = 'product_uid')\n",
    "positive_df = positive_df.merge(df_queries[['search_term', 'query_id']], how = 'left', on = 'query_id')\n",
    "positive_df['search_query'] = [prompt + str(x) for x in positive_df['search_term']]\n",
    "\n",
    "train_data, test_data = train_test_split(positive_df, test_size=0.1, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_examples = [\n",
    "    InputExample(texts=[t1, t2])\n",
    "    for t1, t2 in zip(\n",
    "        train_data['search_query'], train_data['product_text_string']\n",
    "    )\n",
    "]\n",
    "\n",
    "loader = datasets.NoDuplicatesDataLoader(\n",
    "    train_examples, batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take positive pairs and create random negative pairs from the test set\n",
    "pos_test = test_data[['product_text_string', 'search_query']].reset_index(drop=True).copy()\n",
    "pos_test['similarity'] = 1\n",
    "\n",
    "np.random.seed(12)\n",
    "pos_test['rand_merge_1'] = np.random.choice(range(len(pos_test)), size=len(pos_test), replace=False)\n",
    "pos_test['rand_merge_2'] = np.random.choice(range(len(pos_test)), size=len(pos_test), replace=False)\n",
    "\n",
    "neg_test = pos_test[['product_text_string', 'rand_merge_1']].merge(pos_test[['search_query', 'rand_merge_2']], how = 'left', left_on = 'rand_merge_1', right_on = 'rand_merge_2')\n",
    "neg_test['similarity'] = 0\n",
    "\n",
    "pos_neg_test = pd.concat([pos_test, neg_test])\n",
    "pos_neg_test = pos_neg_test.sample(frac=1)\n",
    "\n",
    "\n",
    "evaluator = evaluation.EmbeddingSimilarityEvaluator(\n",
    "    list(pos_neg_test[\"search_query\"]),\n",
    "    list(pos_neg_test[\"product_text_string\"]),\n",
    "    [float(x) for x in pos_neg_test[\"similarity\"]],\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Baseline and Fine-Tune Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['WANDB_DISABLED'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at Snowflake/snowflake-arctic-embed-m and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: BertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "transformer = models.Transformer('Snowflake/snowflake-arctic-embed-m')\n",
    "\n",
    "pooler = models.Pooling(\n",
    "    transformer.get_word_embedding_dimension(),\n",
    "    pooling_mode_mean_tokens=True\n",
    ")\n",
    "\n",
    "model = SentenceTransformer(\n",
    "    modules=[transformer, pooler],\n",
    "    device=device\n",
    ")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_similarity = evaluator(model)\n",
    "pretrain_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{'pearson_cosine': 0.8764524487588595,\n",
    " 'spearman_cosine': 0.8487036957203093,\n",
    " 'pearson_manhattan': 0.828606164662169,\n",
    " 'spearman_manhattan': 0.8285303972567875,\n",
    " 'pearson_euclidean': 0.8315322713504393,\n",
    " 'spearman_euclidean': 0.83022571909546,\n",
    " 'pearson_dot': 0.8748335656385433,\n",
    " 'spearman_dot': 0.8492356027098478,\n",
    " 'pearson_max': 0.8764524487588595,\n",
    " 'spearman_max': 0.8492356027098478}```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = losses.MultipleNegativesRankingLoss(model)\n",
    "\n",
    "epochs = 10\n",
    "warmup_steps = int(len(loader) * epochs * 0.1)\n",
    "\n",
    "num_steps_per_epoch = len(loader)\n",
    "total_steps = (len(loader) / batch_size) * epochs\n",
    "eval_steps = num_steps_per_epoch // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    train_objectives=[(loader, loss)],\n",
    "    epochs=epochs,\n",
    "    evaluator=evaluator,\n",
    "    evaluation_steps=eval_steps,\n",
    "    warmup_steps=warmup_steps,\n",
    "    show_progress_bar=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Step  | Training Loss | Validation Loss | Pearson Cosine | Spearman Cosine | Pearson Manhattan | Spearman Manhattan | Pearson Euclidean | Spearman Euclidean | Pearson Dot | Spearman Dot | Pearson Max | Spearman Max |\n",
    "|-------|---------------|-----------------|----------------|-----------------|-------------------|--------------------|-------------------|--------------------|-------------|--------------|-------------|--------------|\n",
    "| 806   | 0.135900      | No log          | 0.924130       | 0.859629        | 0.906357          | 0.858088           | 0.907278          | 0.858170           | 0.916731    | 0.859120     | 0.924130    | 0.859629     |\n",
    "| 1612  | 0.063700      | No log          | 0.930849       | 0.860865        | 0.915001          | 0.860061           | 0.915904          | 0.860171           | 0.921644    | 0.860218     | 0.930849    | 0.860865     |\n",
    "| 2418  | 0.049000      | No log          | 0.933863       | 0.861664        | 0.918908          | 0.861364           | 0.919644          | 0.861518           | 0.924173    | 0.860963     | 0.933863    | 0.861664     |\n",
    "| 3224  | 0.017200      | No log          | 0.935362       | 0.861663        | 0.919747          | 0.861529           | 0.920262          | 0.861535           | 0.926153    | 0.861065     | 0.935362    | 0.861663     |\n",
    "| 4030  | 0.007400      | No log          | 0.935997       | 0.862507        | 0.919293          | 0.862325           | 0.919973          | 0.862370           | 0.926513    | 0.861935     | 0.935997    | 0.862507     |\n",
    "| 4836  | 0.006000      | No log          | 0.936650       | 0.862111        | 0.919439          | 0.861762           | 0.920107          | 0.861893           | 0.927554    | 0.861589     | 0.936650    | 0.862111     |\n",
    "| 5642  | 0.004000      | No log          | 0.937880       | 0.862215        | 0.920892          | 0.862062           | 0.921706          | 0.862196           | 0.929511    | 0.861718     | 0.937880    | 0.862215     |\n",
    "| 6448  | 0.003300      | No log          | 0.938330       | 0.862047        | 0.921047          | 0.861927           | 0.921739          | 0.862072           | 0.930045    | 0.861569     | 0.938330    | 0.862072     |\n",
    "| 7254  | 0.002100      | No log          | 0.936754       | 0.862297        | 0.918734          | 0.862152           | 0.919268          | 0.862269           | 0.927864    | 0.861827     | 0.936754    | 0.862297     |\n",
    "| 8060  | 0.001500      | No log          | 0.938134       | 0.861851        | 0.920651          | 0.861894           | 0.921173          | 0.862000           | 0.928700    | 0.861206     | 0.938134    | 0.862000     |\n",
    "| 8866  | 0.001500      | No log          | 0.938488       | 0.862100        | 0.920412          | 0.862171           | 0.921088          | 0.862335           | 0.930219    | 0.861655     | 0.938488    | 0.862335     |\n",
    "| 9672  | 0.001300      | No log          | 0.939361       | 0.861918        | 0.921635          | 0.862023           | 0.922244          | 0.862159           | 0.930565    | 0.861448     | 0.939361    | 0.862159     |\n",
    "| 10478 | 0.001000      | No log          | 0.939014       | 0.862050        | 0.920826          | 0.862177           | 0.921358          | 0.862278           | 0.930214    | 0.861611     | 0.939014    | 0.862278     |\n",
    "| 11284 | 0.001500      | No log          | 0.939657       | 0.861916        | 0.921790          | 0.862133           | 0.922281          | 0.862239           | 0.930523    | 0.861401     | 0.939657    | 0.862239     |\n",
    "| 12090 | 0.000600      | No log          | 0.939410       | 0.862178        | 0.921517          | 0.862399           | 0.922049          | 0.862521           | 0.930494    | 0.861686     | 0.939410    | 0.862521     |\n",
    "| 12896 | 0.000800      | No log          | 0.938943       | 0.861639        | 0.920460          | 0.862017           | 0.920930          | 0.862122           | 0.930085    | 0.861131     | 0.938943    | 0.862122     |\n",
    "| 13702 | 0.000400      | No log          | 0.938404       | 0.861748        | 0.919368          | 0.862102           | 0.919783          | 0.862244           | 0.929538    | 0.861267     | 0.938404    | 0.862244     |\n",
    "| 14508 | 0.000400      | No log          | 0.938728       | 0.861622        | 0.919922          | 0.862018           | 0.920329          | 0.862139           | 0.929563    | 0.861129     | 0.938728    | 0.862139     |\n",
    "| 15314 | 0.000300      | No log          | 0.938327       | 0.861611        | 0.919238          | 0.862039           | 0.919665          | 0.862166           | 0.929277    | 0.861128     | 0.938327    | 0.862166     |\n",
    "| 16120 | 0.000300      | No log          | 0.938794       | 0.861668        | 0.919764          | 0.862051           | 0.920197          | 0.862189           | 0.929890    | 0.861168     | 0.938794    | 0.862189     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_similarity = evaluator(model)\n",
    "finetune_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{'pearson_cosine': 0.9387940952734546,\n",
    " 'spearman_cosine': 0.861667612181049,\n",
    " 'pearson_manhattan': 0.9197639425972355,\n",
    " 'spearman_manhattan': 0.8620514864829285,\n",
    " 'pearson_euclidean': 0.9201973979926452,\n",
    " 'spearman_euclidean': 0.8621892011430952,\n",
    " 'pearson_dot': 0.9298899159239588,\n",
    " 'spearman_dot': 0.861167712275619,\n",
    " 'pearson_max': 0.9387940952734546,\n",
    " 'spearman_max': 0.8621892011430952}```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "homedepot",
   "language": "python",
   "name": "homedepot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
